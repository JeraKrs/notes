{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture03 Linear Regression with Multiple Variables\n",
    "\n",
    "\n",
    "## Multivariate Linear Regression\n",
    "\n",
    "**Notation**:\n",
    "* $n$ = number of features\n",
    "* $x^{(i)}$ = input (features) of $i^{th}$ training example.\n",
    "* $x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example.\n",
    "\n",
    "**Hypothesis**: $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n$, for convenicnece of notation, define $x_0 = 1$, then $h_{\\theta}(x) = \\theta^Tx$.\n",
    "\n",
    "**Parameters**: $\\theta$ is n+1 dimensioned vector.\n",
    "\n",
    "**Cost function**: $J(\\theta) = \\frac{1}{2m} \\sum^m_{i=1}(h_{\\theta}(x^{(i)} - y^{(i)})^2$\n",
    "\n",
    "**Gradient descent**: repeat until convergence {$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta  \\theta_j}J(\\theta)$}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Mean Normalization\n",
    "\n",
    "* Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.\n",
    "* Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
    "\n",
    "To implement both of these techniques, adjust the input values as shown in this formula: $x_j := \\frac{x_j - \\mu_j}{s_j}$, where $\\mu_j$ is the average of all the values for feature (j) and $s_j$ is the range of values (max - min), or $s_j$ is the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "Making sure gradient descent is work correctly: $J(\\theta)$ should decrease after every iteration. To summarize:\n",
    "* If $\\alpha$ is too small: slow convergence.\n",
    "* If $\\alpha$ is too large: ï¿¼may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "\n",
    "**Normal equation**: the method to solve for $\\theta$ analytically, $\\theta = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "The following is a comparison of gradient descent and the normal equation (m training examples, n features):\n",
    "\n",
    "* Gradient descent:\n",
    "    * needs to choose $\\alpha$.\n",
    "    * needs many iterations.\n",
    "    * works well even when n is large, $O (kn^2)$\n",
    "* Normal equation:\n",
    "    * doesn't need to choose $\\alpha$.\n",
    "    * doesn't need to iterate.\n",
    "    * slow if n is very large, since it needs to compute $(X^TX)$, $O(n^3)$.\n",
    "    \n",
    "**Noninvertibility**: if $X^TX$ is non-invertible (singular/degenerate)\n",
    "* redundant features.\n",
    "* too many features, in this case, delete some features or use \"regularization\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
